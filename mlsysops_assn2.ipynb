{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0505a39-0c51-457f-ab8a-128a3ba56dac",
   "metadata": {},
   "source": [
    "<h1>Distributed Hyperparameter Optimization (HPO) Techniques for CNN on MNIST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28eb5993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.2.2 (from torchvision)\n",
      "  Using cached torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Collecting filelock (from torch==2.2.2->torchvision)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (4.12.2)\n",
      "Collecting sympy (from torch==2.2.2->torchvision)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.2.2->torchvision)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.1.6)\n",
      "Collecting fsspec (from torch==2.2.2->torchvision)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchvision) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.2->torchvision)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "Using cached torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed filelock-3.18.0 fsspec-2025.3.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 torch-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40378dfd-c7b5-4245-a20b-27c4024d8854",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>\n",
    "\n",
    "Hyperparameter Optimization (HPO) is a critical step in deep learning model training to improve accuracy and efficiency. \n",
    "Traditional hyperparameter tuning approaches like Grid Search and Random Search are computationally expensive and inefficient. \n",
    "\n",
    "In this assignment, we compare and analyze different hyperparameter optimization strategies using distributed computing to achieve optimal hyperparameter selection efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f8c43-f571-4fa8-9035-b4872b8a1a31",
   "metadata": {},
   "source": [
    "<h2>2. Objectives</h2>\n",
    "\n",
    "The goal of this project is to:\n",
    "\n",
    "1. Compare multiple HPO techniques for training a Convolutional Neural Network (CNN) on the MNIST dataset.\n",
    "\n",
    "2. Evaluate these techniques based on training speed, search efficiency, accuracy, and GPU resource utilization.\n",
    "\n",
    "3. Implement real-time GPU monitoring to track memory usage and optimize resource allocation.\n",
    "\n",
    "4. Identify the most effective HPO method that balances speed, accuracy, and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f276fa1-ccda-4598-8a80-742c5c02154f",
   "metadata": {},
   "source": [
    "<h2>3. HPO Strategies Implemented</h2>\n",
    "\n",
    "We implemented and compared four different approaches for HPO:\n",
    "\n",
    "1. Baseline (No HPO): Train the model with default hyperparameters.\n",
    "\n",
    "2. ASHA (Asynchronous Successive Halving Algorithm): Eliminates underperforming trials early to speed up training.\n",
    "\n",
    "3. BOHB (Bayesian Optimization + HyperBand): Uses Bayesian learning to intelligently select hyperparameters while efficiently allocating compute resources.\n",
    "\n",
    "4. BOHB + ASHA Hybrid: Combines BOHB’s smart selection with ASHA’s aggressive pruning for improved efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a9552-e528-44b8-84da-76ad306fd4c8",
   "metadata": {},
   "source": [
    "<h2>4. Implementation Details</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1af501-d51b-40a7-828d-a80e561c4a49",
   "metadata": {},
   "source": [
    "<h2>4.1 Dataset: MNIST</h2>\n",
    "\n",
    "The MNIST dataset consists of handwritten digits (0-9).\n",
    "\n",
    "Training set: 1000 images.\n",
    "\n",
    "Test set: 1000 images.\n",
    "\n",
    "Image size: 28x28 pixels, grayscale.\n",
    "\n",
    "Output classes: 10 (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c712bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import psutil  # For system memory tracking\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import psutil  # For CPU memory tracking\n",
    "import time\n",
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner  # ASHA Implementation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c1806f-471a-46b3-bc51-4c009c826047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load full MNIST dataset\n",
    "full_trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "full_testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Select 1000 random indices for train and test sets\n",
    "train_indices = np.random.choice(len(full_trainset), 10000, replace=False)\n",
    "test_indices = np.random.choice(len(full_testset), 10000, replace=False)\n",
    "\n",
    "# Create subsets of MNIST\n",
    "trainset = Subset(full_trainset, train_indices)\n",
    "testset = Subset(full_testset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "dataset = (trainloader, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9196b-0d2d-43cd-85fe-4a1a4ba4aa7f",
   "metadata": {},
   "source": [
    "<h2>4.2 Model: CNN Architecture</h2>\n",
    "\n",
    "The CNN model used for training consists of:\n",
    "\n",
    "1. Two convolutional layers with ReLU activation.\n",
    "\n",
    "2. Max-pooling layers for feature down-sampling.\n",
    "\n",
    "3. Fully connected layers with a dropout layer.\n",
    "\n",
    "4. Softmax activation for classification.\n",
    "\n",
    "<b>Hyperparameters Considered</b>\n",
    "\n",
    "Learning Rate - 1e-4 to 1e-2 (log scale)\n",
    "\n",
    "Dropout Rate - 0.2 to 0.5\n",
    "\n",
    "Number of Filters - 16, 32, 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddb55b1-1211-487b-878a-cafaef7b7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CNN Model for MNIST\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5, num_filters=32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(num_filters * 2 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabadb6d-8c84-44e4-a84f-0f29d235a66e",
   "metadata": {},
   "source": [
    "<h2>4.3 GPU Monitoring & Resource Utilization Tracking</h2>\n",
    "\n",
    "We implemented real-time GPU monitoring using PyTorch’s memory allocation tracking.\n",
    "\n",
    "GPU usage was recorded at each training epoch.\n",
    "\n",
    "This allowed us to compare memory efficiency across different HPO techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23cea0c-590e-4a5e-8742-dc0766bc902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Monitor GPU Memory Usage\n",
    "# def log_gpu_usage(tag=\"\"):\n",
    "#     if torch.cuda.is_available():\n",
    "#         memory_allocated = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
    "#         memory_reserved = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
    "#         print(f\"[{tag}] GPU Memory Used: {memory_allocated:.2f} MB, Reserved: {memory_reserved:.2f} MB\")\n",
    "#         return memory_allocated\n",
    "#     return 0  # Return 0 if no GPU available\n",
    "\n",
    "\n",
    "# Function to log memory usage (CPU + GPU approximation)\n",
    "def log_memory_usage(stage=\"\"):\n",
    "    # Get CPU RAM usage\n",
    "    ram_usage = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "    \n",
    "    # Get GPU memory (Approximate via tensor usage)\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()  # Free unused memory (for better tracking)\n",
    "        gpu_usage = \"MPS does not expose memory tracking\"\n",
    "    else:\n",
    "        gpu_usage = \"GPU not in use\"\n",
    "    \n",
    "    return ram_usage, gpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3034e26-fe0d-454d-afb6-fe1023814bb7",
   "metadata": {},
   "source": [
    "<h2>5. Comparison of HPO Approaches</h2>\n",
    "\n",
    "1. Training Speed\n",
    "2. Model Accuracy\n",
    "3. GPU Memory Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5b13a-4008-4923-863e-9a3beaba337c",
   "metadata": {},
   "source": [
    "<h2>5.1 Baseline Model (No HPO)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c14589a-da69-4c1d-92c5-6e2f2e515221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7400a6d-ddfa-45db-b214-ce6d0d6d13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Baseline Accuracy: 97.86%, Training Time: 46.62s, Avg CPU RAM Usage: 8572.09 MB, GPU Usage: MPS does not expose memory tracking\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train Baseline Model (Without HPO) with GPU Logging\n",
    "def train_baseline():\n",
    "    writer = SummaryWriter(log_dir=\"./logs/baseline\")\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = CNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # gpu_usages = []\n",
    "    memory_logs = []\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log GPU Memory\n",
    "        # gpu_usage = log_gpu_usage(\"Baseline\")\n",
    "        # gpu_usages.append(gpu_usage)\n",
    "        \n",
    "        # Log Memory Usage\n",
    "        ram_usage, gpu_usage = log_memory_usage(\"Baseline\")\n",
    "        memory_logs.append(ram_usage)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss / len(trainloader), epoch)\n",
    "        writer.add_scalar(\"Memory/CPU_RAM_MB\", ram_usage, epoch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compute GPU Usage Stats\n",
    "    # avg_gpu_usage = sum(gpu_usages) / len(gpu_usages)\n",
    "    # avg_gpu_usage = \"MPS memory tracking unavailable\"\n",
    "\n",
    "    # Compute Average Memory Usage\n",
    "    avg_ram_usage = sum(memory_logs) / len(memory_logs)\n",
    "\n",
    "    # Test Model\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Baseline Accuracy: {accuracy:.2f}%, Training Time: {end_time - start_time:.2f}s, Avg CPU RAM Usage: {avg_ram_usage:.2f} MB, GPU Usage: {gpu_usage}\")\n",
    "    writer.close()\n",
    "\n",
    "    # return accuracy, end_time - start_time, avg_gpu_usage\n",
    "    return accuracy, end_time - start_time, avg_ram_usage\n",
    "    \n",
    "\n",
    "# Run Baseline Training\n",
    "# baseline_accuracy, baseline_time, baseline_gpu = train_baseline()\n",
    "baseline_accuracy, baseline_time, baseline_memory = train_baseline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ca153-a9fa-4683-80c0-b9f59093fdc0",
   "metadata": {},
   "source": [
    "<h2>5.2 ASHA HPO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b7bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.39-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/abhinav.gazta/Library/Python/3.11/lib/python/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
      "Downloading sqlalchemy-2.0.39-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script mako-render is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script alembic is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script optuna is installed in '/Library/Frameworks/Python.framework/Versions/3.11/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 greenlet-3.1.1 optuna-4.2.1 sqlalchemy-2.0.39 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101f417-fab2-4a62-ad18-ff0b728ac423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train Model with ASHA HPO, Memory Logging, and TensorBoard Logging\n",
    "def train_cnn_asha(trial):\n",
    "    writer = SummaryWriter(log_dir=f\"./logs/asha_trial_{trial.number}\")  # TensorBoard log directory\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Sample hyperparameters using Optuna\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    num_filters = trial.suggest_categorical(\"num_filters\", [16, 32, 64])\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = CNN(dropout_rate=dropout_rate, num_filters=num_filters).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    memory_logs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Log Memory Usage\n",
    "        ram_usage, gpu_usage = log_memory_usage(\"ASHA\")\n",
    "        memory_logs.append(ram_usage)\n",
    "\n",
    "        # Log Loss and Memory to TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss / len(trainloader), epoch)\n",
    "        writer.add_scalar(\"Memory/CPU_RAM_MB\", ram_usage, epoch)\n",
    "\n",
    "        # Evaluate Model (ASHA needs validation accuracy for pruning)\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Report accuracy for ASHA pruning\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # ASHA: Stop bad trials early\n",
    "        if trial.should_prune():\n",
    "            writer.close()  # Ensure writer closes even when pruned\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Compute Average CPU Memory Usage\n",
    "    avg_ram_usage = sum(memory_logs) / len(memory_logs)\n",
    "\n",
    "    # Log final accuracy and memory stats to TensorBoard\n",
    "    writer.add_scalar(\"Accuracy\", accuracy)\n",
    "    writer.add_scalar(\"Training Time (s)\", end_time - start_time)\n",
    "    writer.close()\n",
    "\n",
    "    # Print Summary (Same Format as Baseline)\n",
    "    print(f\"Accuracy: {accuracy:.2f}%, Training Time: {end_time - start_time:.2f}s, \"\n",
    "          f\"Avg CPU RAM Usage: {avg_ram_usage:.2f} MB, GPU Usage: {gpu_usage}\")\n",
    "\n",
    "    return accuracy, end_time - start_time, avg_ram_usage, gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee6251-1944-46b2-ac6e-2bb314f192d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``constant_liar`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-03-18 11:03:43,113] A new study created in memory with name: asha_hpo\n",
      "[I 2025-03-18 11:04:38,575] Trial 0 finished with value: 93.83 and parameters: {'dropout': 0.34445838191119016, 'num_filters': 16, 'lr': 0.0001481271872072106}. Best is trial 0 with value: 93.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.83%, Training Time: 54.50s, Avg CPU RAM Usage: 8456.52 MB, GPU Usage: MPS does not expose memory tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-18 11:05:28,606] Trial 1 finished with value: 96.25 and parameters: {'dropout': 0.3892764670419021, 'num_filters': 16, 'lr': 0.0003344461352061419}. Best is trial 1 with value: 96.25.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.25%, Training Time: 50.02s, Avg CPU RAM Usage: 8411.15 MB, GPU Usage: MPS does not expose memory tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-18 11:06:22,065] Trial 2 finished with value: 97.32 and parameters: {'dropout': 0.3380726923209525, 'num_filters': 32, 'lr': 0.0043144527413316106}. Best is trial 2 with value: 97.32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.32%, Training Time: 53.45s, Avg CPU RAM Usage: 8333.52 MB, GPU Usage: MPS does not expose memory tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-18 11:07:15,216] Trial 3 finished with value: 97.66 and parameters: {'dropout': 0.4112362766199672, 'num_filters': 16, 'lr': 0.008400365048314784}. Best is trial 3 with value: 97.66.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.66%, Training Time: 53.14s, Avg CPU RAM Usage: 8354.39 MB, GPU Usage: MPS does not expose memory tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-18 11:07:36,303] Trial 4 pruned. \n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "import multiprocessing\n",
    "\n",
    "# Store best training time & resource usage\n",
    "best_training_time = float(\"inf\")\n",
    "best_ram_usage = float(\"inf\")\n",
    "best_gpu_usage = None\n",
    "\n",
    "# Optimize parallel processing\n",
    "# n_jobs = max(1, multiprocessing.cpu_count() // 2)  # Use half the available cores\n",
    "\n",
    "# Enable best GPU performance for Apple MPS\n",
    "torch.set_float32_matmul_precision('high') \n",
    "\n",
    "# Define Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    global best_training_time, best_ram_usage, best_gpu_usage\n",
    "\n",
    "    accuracy, training_time, avg_ram_usage, gpu_usage = train_cnn_asha(trial)  # Now returns more metrics\n",
    "\n",
    "    # Track best training time & resource utilization\n",
    "    if training_time < best_training_time:\n",
    "        best_training_time = training_time\n",
    "        best_ram_usage = avg_ram_usage\n",
    "        best_gpu_usage = gpu_usage\n",
    "\n",
    "    return accuracy  # Optuna optimizes based on accuracy\n",
    "\n",
    "# Create Optuna Study with ASHA (Successive Halving)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"asha_hpo\",\n",
    "    direction=\"maximize\",  # We want to maximize accuracy\n",
    "    pruner=SuccessiveHalvingPruner(),  # ASHA Pruning\n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        multivariate=True,  # Optimizes multiple parameters together\n",
    "        constant_liar=True  # Avoids redundant evaluations\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run Optimization (20 Trials)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print Best Results\n",
    "print(f\"\\nBest Model Config: {study.best_params}\")\n",
    "print(f\"Best Accuracy: {study.best_value:.2f}%\")\n",
    "print(f\"Best Training Time: {best_training_time:.2f}s\")\n",
    "print(f\"Best Avg CPU RAM Usage: {best_ram_usage:.2f} MB\")\n",
    "print(f\"Best GPU Usage: {best_gpu_usage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8056fc-b5af-4ecf-b9aa-72db7d07e700",
   "metadata": {},
   "source": [
    "<h2>5.3 Train with BOHB HPO</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c54e3435-9bd7-4d3a-aa9f-273b68001603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import psutil\n",
    "import time\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers.bohb import BOHB\n",
    "from hpbandster.core.worker import Worker\n",
    "import ConfigSpace as CS\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Define Worker for BOHB\n",
    "class CNNWorker(Worker):\n",
    "    def __init__(self, run_id, dataset, **kwargs):\n",
    "        # print('__init__')\n",
    "        super().__init__(run_id, **kwargs)\n",
    "        self.trainloader, self.testloader = dataset\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    def compute(self, config, budget, **kwargs):\n",
    "        print(type(config))\n",
    "        print(type(budget))\n",
    "        writer = SummaryWriter(log_dir=f\"./logs/bohb_trial_{config}\")\n",
    "        print('writer')\n",
    "\n",
    "        # ✅ Convert `config` to Python native dict\n",
    "        config_native = {key: int(value) if isinstance(value, np.integer) else float(value) if isinstance(value, np.floating) else value for key, value in config.items()}\n",
    "        \n",
    "        model = CNN(dropout_rate=float(config[\"dropout\"]), num_filters=int(config[\"num_filters\"])).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=float(config[\"lr\"]))\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        print('reached loss_fn')\n",
    "\n",
    "        memory_logs = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(int(budget)):  # `budget` is set by BOHB (early stopping)\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for images, labels in self.trainloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Log Memory Usage\n",
    "            ram_usage, gpu_usage = log_memory_usage()\n",
    "            memory_logs.append(float(ram_usage))\n",
    "\n",
    "            # Log Loss and Memory to TensorBoard\n",
    "            writer.add_scalar(\"Loss/train\", epoch_loss / len(self.trainloader), epoch)\n",
    "            writer.add_scalar(\"Memory/CPU_RAM_MB\", ram_usage, epoch)\n",
    "\n",
    "        end_time = time.time()\n",
    "        avg_ram_usage = sum(memory_logs) / len(memory_logs)\n",
    "\n",
    "        # Evaluate Model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.testloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # ✅ Convert all NumPy types to standard Python types before returning\n",
    "        accuracy = int(np.round(accuracy))  # Convert NumPy int64 to Python int\n",
    "        avg_ram_usage = float(np.round(avg_ram_usage, 2))  # Convert float32 to Python float\n",
    "        training_time = float(np.round(end_time - start_time, 2))  # Convert time to Python float\n",
    "\n",
    "        writer.add_scalar(\"Accuracy\", accuracy)\n",
    "        writer.add_scalar(\"Training Time (s)\", training_time)\n",
    "        writer.close()\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}%, Training Time: {training_time:.2f}s, \"\n",
    "              f\"Avg CPU RAM Usage: {avg_ram_usage:.2f} MB, GPU Usage: {gpu_usage}\")\n",
    "\n",
    "        return {\n",
    "        \"loss\": -accuracy,  # Loss should be negative for BOHB to maximize accuracy\n",
    "        \"info\": {\n",
    "            \"training_time\": training_time,\n",
    "            \"ram_usage\": avg_ram_usage,\n",
    "            \"config\": config_native  # ✅ Ensure all values are JSON serializable\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "        cs = CS.ConfigurationSpace()\n",
    "        cs.add(CS.UniformFloatHyperparameter(\"dropout\", lower= float(0.2), upper= float(0.5)))\n",
    "        cs.add(CS.CategoricalHyperparameter(\"num_filters\", choices=[16, 32, 64]))\n",
    "        cs.add(CS.UniformFloatHyperparameter(\"lr\", lower=float(0.0001), upper=float(0.01)))\n",
    "        # print('CS')\n",
    "        return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "505516ff-3c9d-4b15-a816-e0b0359a4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configspace():\n",
    "    config_space = {\n",
    "        \"dropout\": {\"type\": \"float\", \"lower\": 0.2, \"upper\": 0.5},\n",
    "        \"num_filters\": {\"type\": \"categorical\", \"choices\": [16, 32, 64]},\n",
    "        \"lr\": {\"type\": \"float\", \"lower\": 0.0001, \"upper\": 0.01}\n",
    "    }\n",
    "    return config_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80156c87-10b9-4728-9fba-43fb0346ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_hyperparameters(config_space):\n",
    "    sampled_config = {}\n",
    "    for param, details in config_space.items():\n",
    "        if details[\"type\"] == \"float\":\n",
    "            sampled_config[param] = random.uniform(details[\"lower\"], details[\"upper\"])\n",
    "        elif details[\"type\"] == \"categorical\":\n",
    "            sampled_config[param] = random.choice(details[\"choices\"])\n",
    "    return sampled_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d354d37-9ca9-4d1c-a935-fb78a0ec45c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Hyperparameters: {'dropout': 0.37038666386353447, 'num_filters': 32, 'lr': 0.0017675938006139328}\n"
     ]
    }
   ],
   "source": [
    "config_space = get_configspace()\n",
    "sampled_config = sample_hyperparameters(config_space)\n",
    "print(\"Sampled Hyperparameters:\", sampled_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0dc1d3d3-7023-4686-ab66-fbe9535cece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:19:02 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x302e18050; connected IPv4; for PYRO:Pyro.NameServer@localhost:53931>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    dropout, Type: UniformFloat, Range: [0.2, 0.5], Default: 0.35\n",
      "    lr, Type: UniformFloat, Range: [0.0001, 0.01], Default: 0.00505\n",
      "    num_filters, Type: Categorical, Choices: {16, 32, 64}, Default: 16\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_hyperparameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(CNNWorker.get_configspace())\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run BOHB Optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m bohb = \u001b[43mBOHB\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfigspace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampled_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnameserver\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnameserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_budget\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Minimum epochs per trial\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_budget\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Maximum epochs per trial\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# result_logger=result_logger\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m res = bohb.run(n_iterations= \u001b[38;5;28mint\u001b[39m(\u001b[32m20\u001b[39m))  \u001b[38;5;66;03m# Number of trials\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# # Shutdown Nameserver and Worker\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# bohb.shutdown(shutdown_workers=True)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# NS.shutdown()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# print(f\"\\n🔹 Best Model Config: {res.get_id2config_mapping()[best_config]['config']}\")\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# print(f\"✅ Best Accuracy: {best_accuracy:.2f}%\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/BITS/mlsop/lib/python3.11/site-packages/hpbandster/optimizers/bohb.py:85\u001b[39m, in \u001b[36mBOHB.__init__\u001b[39m\u001b[34m(self, configspace, eta, min_budget, max_budget, min_points_in_model, top_n_percent, num_samples, random_fraction, bandwidth_factor, min_bandwidth, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m configspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     81\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to provide a valid CofigSpace object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m cg = \u001b[43mCG_BOHB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigspace\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mmin_points_in_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_points_in_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mtop_n_percent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n_percent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mrandom_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_fraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mbandwidth_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbandwidth_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mmin_bandwidth\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_bandwidth\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config_generator=cg, **kwargs)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Hyperband related stuff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/BITS/mlsop/lib/python3.11/site-packages/hpbandster/optimizers/config_generators/bohb.py:56\u001b[39m, in \u001b[36mBOHB.__init__\u001b[39m\u001b[34m(self, configspace, min_points_in_model, top_n_percent, num_samples, random_fraction, bandwidth_factor, min_bandwidth, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m.min_points_in_model = min_points_in_model\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_points_in_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \t\u001b[38;5;28mself\u001b[39m.min_points_in_model = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfigspace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_hyperparameters\u001b[49m())+\u001b[32m1\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_points_in_model < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.configspace.get_hyperparameters())+\u001b[32m1\u001b[39m:\n\u001b[32m     59\u001b[39m \t\u001b[38;5;28mself\u001b[39m.logger.warning(\u001b[33m'\u001b[39m\u001b[33mInvalid min_points_in_model value. Setting it to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m'\u001b[39m%(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.configspace.get_hyperparameters())+\u001b[32m1\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'get_hyperparameters'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:19:02 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "04:19:02 WORKER: start listening for jobs\n"
     ]
    }
   ],
   "source": [
    "# Set up BOHB optimization\n",
    "run_id = \"bohb_hpo\"\n",
    "# result_logger = hpres.json_result_logger(directory=\"./bohb_results\", overwrite=True)\n",
    "\n",
    "# Start Nameserver for BOHB\n",
    "NS = hpns.NameServer(run_id=run_id, host=\"localhost\", port=0)\n",
    "NS.start()\n",
    "\n",
    "# Start BOHB Worker\n",
    "worker = CNNWorker(run_id=run_id, dataset=dataset, nameserver=\"localhost\", nameserver_port=NS.port)\n",
    "worker.run(background=True)\n",
    "\n",
    "print(CNNWorker.get_configspace())\n",
    "\n",
    "# Run BOHB Optimization\n",
    "bohb = BOHB(\n",
    "    configspace=sampled_config,\n",
    "    run_id=run_id,\n",
    "    nameserver=\"localhost\",\n",
    "    nameserver_port=NS.port,\n",
    "    min_budget= int(1),  # Minimum epochs per trial\n",
    "    max_budget= int(5)  # Maximum epochs per trial\n",
    "    # result_logger=result_logger\n",
    ")\n",
    "\n",
    "res = bohb.run(n_iterations= int(20))  # Number of trials\n",
    "\n",
    "# # Shutdown Nameserver and Worker\n",
    "# bohb.shutdown(shutdown_workers=True)\n",
    "# NS.shutdown()\n",
    "\n",
    "# # Get Best Hyperparameters\n",
    "# best_config = res.get_incumbent_id()\n",
    "# best_accuracy = -res.get_incumbent_trajectory()[\"losses\"][-1]\n",
    "\n",
    "# print(f\"\\n🔹 Best Model Config: {res.get_id2config_mapping()[best_config]['config']}\")\n",
    "# print(f\"✅ Best Accuracy: {best_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
